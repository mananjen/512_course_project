{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Layer, Input, UpSampling2D, Concatenate, Reshape, Lambda\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class SpatialRNNCell(Layer):\n",
    "    def __init__(self, direction, **kwargs):\n",
    "        super(SpatialRNNCell, self).__init__(**kwargs)\n",
    "        self.direction = direction\n",
    "\n",
    "    def roll_output(self, inputs, direction):\n",
    "        shifts = {\"right\": (0, 1), \"left\": (0, -1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
    "        return tf.roll(inputs, shift=shifts[direction], axis=(1, 2))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(SpatialRNNCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        num_steps = tf.shape(inputs)[2] if self.direction in ['left', 'right'] else tf.shape(inputs)[1]\n",
    "        outputs = inputs\n",
    "        for _ in tf.range(num_steps):\n",
    "            outputs = Lambda(lambda x: self.roll_output(x, self.direction))(outputs)\n",
    "            outputs *= self.alpha\n",
    "            outputs = tf.nn.relu(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class AttentionEstimatorNetwork(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionEstimatorNetwork, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv2 = Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv3 = Conv2D(4, (1, 1), activation=None, padding='same')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv1.build(input_shape)\n",
    "        self.conv2.build(self.conv1.compute_output_shape(input_shape))\n",
    "        self.conv3.build(self.conv2.compute_output_shape(self.conv1.compute_output_shape(input_shape)))\n",
    "        super(AttentionEstimatorNetwork, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = self.conv1.compute_output_shape(input_shape)\n",
    "        output_shape = self.conv2.compute_output_shape(output_shape)\n",
    "        return self.conv3.compute_output_shape(output_shape)\n",
    "    \n",
    "class DirectionAwareSpatialContextModule(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DirectionAwareSpatialContextModule, self).__init__(**kwargs)\n",
    "        self.attention_network = AttentionEstimatorNetwork()\n",
    "        self.spatial_rnn_cells = {\n",
    "            'right': SpatialRNNCell('right'),\n",
    "            'left': SpatialRNNCell('left'),\n",
    "            'up': SpatialRNNCell('up'),\n",
    "            'down': SpatialRNNCell('down')\n",
    "        }\n",
    "        self.hidden_to_hidden_conv = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention_network.build(input_shape)\n",
    "        for cell in self.spatial_rnn_cells.values():\n",
    "            cell.build(input_shape)\n",
    "        num_channels = self.attention_network.compute_output_shape(input_shape)[-1]\n",
    "        self.hidden_to_hidden_conv = Conv2D(num_channels // 4, (1, 1), activation='relu')\n",
    "        self.hidden_to_hidden_conv.build(input_shape)\n",
    "        super(DirectionAwareSpatialContextModule, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_weights = self.attention_network(inputs)\n",
    "        attention_weights_split = tf.split(attention_weights, num_or_size_splits=4, axis=-1)\n",
    "        \n",
    "        context_features = []\n",
    "        for direction, rnn_cell in self.spatial_rnn_cells.items():\n",
    "            rnn_output = rnn_cell(inputs)\n",
    "            weighted_feature = rnn_output * attention_weights_split.pop(0)\n",
    "            context_features.append(weighted_feature)\n",
    "\n",
    "        concatenated_features = tf.concat(context_features, axis=-1)\n",
    "        output_dsc_features = self.hidden_to_hidden_conv(concatenated_features)\n",
    "        return output_dsc_features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = (input_shape[0], input_shape[1], input_shape[2], input_shape[3] // 4)\n",
    "        return output_shape\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the VGG16 model\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Initialize the DirectionAwareSpatialContextModule\n",
    "dascm = DirectionAwareSpatialContextModule()\n",
    "\n",
    "# Define the input for the functional model\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Get the feature maps from all layers of VGG16, except the input layer\n",
    "vgg_outputs = [layer.output for layer in vgg.layers[1:]]  # Skip the input layer\n",
    "\n",
    "# Upsample and apply the DirectionAwareSpatialContextModule to each of the feature maps\n",
    "dascm_outputs = []\n",
    "for output in vgg_outputs:\n",
    "    # The output needs to be upsampled to the same size\n",
    "    # Calculate the upsample size for current feature map\n",
    "    upsample_size = (224 // output.shape[1], 224 // output.shape[2])\n",
    "    upsampled_output = UpSampling2D(size=upsample_size)(output)\n",
    "    # Process each upsampled output with the dascm\n",
    "    processed_output = dascm(upsampled_output)\n",
    "    dascm_outputs.append(processed_output)\n",
    "\n",
    "# Concatenate all the DASC module feature maps\n",
    "# We can concatenate along the channel axis as the spatial dimensions are now equal\n",
    "concatenated_outputs = Concatenate(axis=-1)(dascm_outputs)\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=input_tensor, outputs=concatenated_outputs)\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the model summary to check if everything is connected properly\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3268 images belonging to 1 classes.\n",
      "Found 3268 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Define paths to the datasets\n",
    "image_dir = 'E:/CS 512 Project/Proj/shadow/SBU-shadow/SBUTrain4KRecoveredSmall/ShadowImages'\n",
    "mask_dir = 'E:/CS 512 Project/Proj/shadow/SBU-shadow/SBUTrain4KRecoveredSmall/ShadowMasks'\n",
    "\n",
    "# Initialize the data generators\n",
    "image_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "mask_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Connect the generators to your dataset\n",
    "# Since we are not classifying images, we set class_mode to None\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # No class labels\n",
    "    subset='training',\n",
    "    seed=1)\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    mask_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # No class labels\n",
    "    color_mode='grayscale',  # Masks are grayscale\n",
    "    subset='training',\n",
    "    seed=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    # Calculate the weight for each class\n",
    "    Np = tf.reduce_sum(y_true)  # Number of positive samples\n",
    "    Nn = tf.reduce_sum(1 - y_true)  # Number of negative samples\n",
    "    TN = tf.math.count_nonzero(y_pred * (1 - y_true))\n",
    "    TP = tf.math.count_nonzero((1 - y_pred) * y_true)\n",
    "    \n",
    "    # Weights for balancing the classes\n",
    "    weight_for_0 = (Nn / (Nn + Np)) * (1 / (1 - TN / Nn))\n",
    "    weight_for_1 = (Np / (Nn + Np)) * (1 / (1 - TP / Np))\n",
    "    \n",
    "    # Calculate the actual weighted cross-entropy\n",
    "    # The '+ epsilon()' part is to avoid log(0)\n",
    "    loss = -(weight_for_1 * y_true * tf.math.log(y_pred + tf.keras.backend.epsilon()) + \n",
    "             weight_for_0 * (1 - y_true) * tf.math.log(1 - y_pred + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=weighted_cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m2189501697936\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\\n  • training=True\\n  • mask=None'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(image_dir)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or however many you choose\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Manan\\anaconda3\\envs\\proj\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Manan\\anaconda3\\envs\\proj\\Lib\\site-packages\\keras\\src\\ops\\function.py:159\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[1;34m(self, inputs, operation_fn)\u001b[0m\n\u001b[0;32m    157\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[1;32m--> 159\u001b[0m     output_tensors\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtensor_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mpack_sequence_as(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_struct, output_tensors)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m2189501697936\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\\n  • training=True\\n  • mask=None'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def generate_train_batches(image_generator, mask_generator):\n",
    "    while True:\n",
    "        # Get next image and mask batch\n",
    "        image_batch = next(image_generator)\n",
    "        mask_batch = next(mask_generator)\n",
    "        yield (image_batch, mask_batch)\n",
    "\n",
    "# Create a generator\n",
    "train_generator = generate_train_batches(image_generator, mask_generator)\n",
    "\n",
    "# Ensure that image_dir points directly to the folder containing the images\n",
    "steps_per_epoch = math.ceil(len(os.listdir(image_dir)) / 32)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50  # or however many you choose\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (32, 224, 224, 3)\n",
      "Mask batch shape: (32, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the output of the generator\n",
    "image_batch, mask_batch = next(train_generator)\n",
    "print('Image batch shape:', image_batch.shape)\n",
    "print('Mask batch shape:', mask_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
