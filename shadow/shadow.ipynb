{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SpatialRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, direction, **kwargs):\n",
    "        super(SpatialRNNCell, self).__init__(**kwargs)\n",
    "        self.direction = direction\n",
    "        # In the literature, alpha is initialized as an identity matrix, but for simplicity, \n",
    "        # we're initializing it as a scalar which will have an equivalent effect of scaling the outputs.\n",
    "        self.alpha = self.add_weight(shape=(1,), initializer=\"ones\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Identify the shape of the input feature map.\n",
    "        feature_map_shape = tf.shape(inputs)\n",
    "        height = feature_map_shape[1]\n",
    "        width = feature_map_shape[2]\n",
    "\n",
    "        # Determine the number of recurrent steps based on the direction.\n",
    "        num_steps = width if self.direction in ['left', 'right'] else height\n",
    "        \n",
    "        # Initialize the output with the input values.\n",
    "        outputs = inputs\n",
    "        \n",
    "        # Perform the recurrent translation.\n",
    "        for _ in range(num_steps):\n",
    "            outputs = tf.roll(outputs, shift={'right': (0, 1), 'left': (0, -1),\n",
    "                                              'up': (-1, 0), 'down': (1, 0)}[self.direction], axis=(1, 2))\n",
    "            # Apply the alpha weight and ReLU activation at each step.\n",
    "            outputs = self.alpha * outputs\n",
    "            outputs = tf.nn.relu(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class AttentionEstimatorNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionEstimatorNetwork, self).__init__(**kwargs)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(4, (1, 1), activation=None, padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n",
    "\n",
    "\n",
    "class DirectionAwareSpatialContextModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DirectionAwareSpatialContextModule, self).__init__(**kwargs)\n",
    "        self.attention_network = AttentionEstimatorNetwork()\n",
    "        self.spatial_rnn_cells = {\n",
    "            direction: SpatialRNNCell(direction=direction)\n",
    "            for direction in ['right', 'left', 'up', 'down']\n",
    "        }\n",
    "        # Layer for the hidden-to-hidden translation\n",
    "        self.hidden_to_hidden_conv = tf.keras.layers.Conv2D(32, (1, 1), activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute attention weights\n",
    "        W = self.attention_network(inputs)\n",
    "        W_split = tf.split(W, num_or_size_splits=4, axis=-1)\n",
    "\n",
    "        # Apply spatial RNN in four directions for the first round\n",
    "        context_features_1st_round = {\n",
    "            direction: rnn_cell(inputs)\n",
    "            for direction, rnn_cell in self.spatial_rnn_cells.items()\n",
    "        }\n",
    "\n",
    "        # Multiply first-round context features with attention weights\n",
    "        weighted_context_features_1st_round = [\n",
    "            context_features_1st_round[dir] * W_split[i]\n",
    "            for i, dir in enumerate(['right', 'left', 'up', 'down'])\n",
    "        ]\n",
    "\n",
    "        # Concatenate features and apply 1x1 convolution to reduce dimensionality\n",
    "        concatenated_features_1st_round = tf.concat(weighted_context_features_1st_round, axis=-1)\n",
    "        reduced_features_1st_round = self.hidden_to_hidden_conv(concatenated_features_1st_round)\n",
    "\n",
    "        # Apply spatial RNN in four directions for the second round\n",
    "        context_features_2nd_round = {\n",
    "            direction: rnn_cell(reduced_features_1st_round)\n",
    "            for direction, rnn_cell in self.spatial_rnn_cells.items()\n",
    "        }\n",
    "\n",
    "        # Multiply second-round context features with the SAME attention weights\n",
    "        weighted_context_features_2nd_round = [\n",
    "            context_features_2nd_round[dir] * W_split[i]\n",
    "            for i, dir in enumerate(['right', 'left', 'up', 'down'])\n",
    "        ]\n",
    "\n",
    "        # Concatenate features from the second round\n",
    "        concatenated_features_2nd_round = tf.concat(weighted_context_features_2nd_round, axis=-1)\n",
    "\n",
    "        # Final 1x1 convolution followed by ReLU as per the literature\n",
    "        output_dsc_features = self.hidden_to_hidden_conv(concatenated_features_2nd_round)\n",
    "\n",
    "        return output_dsc_features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
