{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Layer, Input, UpSampling2D, Concatenate, Reshape, Lambda\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class SpatialRNNCell(Layer):\n",
    "    def __init__(self, direction, **kwargs):\n",
    "        super(SpatialRNNCell, self).__init__(**kwargs)\n",
    "        self.direction = direction\n",
    "\n",
    "    def roll_output(self, inputs, direction):\n",
    "        shifts = {\"right\": (0, 1), \"left\": (0, -1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
    "        return tf.roll(inputs, shift=shifts[direction], axis=(1, 2))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(SpatialRNNCell, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        num_steps = tf.shape(inputs)[2] if self.direction in ['left', 'right'] else tf.shape(inputs)[1]\n",
    "        outputs = inputs\n",
    "        for _ in tf.range(num_steps):\n",
    "            outputs = Lambda(lambda x: self.roll_output(x, self.direction))(outputs)\n",
    "            outputs *= self.alpha\n",
    "            outputs = tf.nn.relu(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class AttentionEstimatorNetwork(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionEstimatorNetwork, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv2 = Conv2D(32, (3, 3), activation='relu', padding='same')\n",
    "        self.conv3 = Conv2D(4, (1, 1), activation=None, padding='same')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv1.build(input_shape)\n",
    "        self.conv2.build(self.conv1.compute_output_shape(input_shape))\n",
    "        self.conv3.build(self.conv2.compute_output_shape(self.conv1.compute_output_shape(input_shape)))\n",
    "        super(AttentionEstimatorNetwork, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = self.conv1.compute_output_shape(input_shape)\n",
    "        output_shape = self.conv2.compute_output_shape(output_shape)\n",
    "        return self.conv3.compute_output_shape(output_shape)\n",
    "    \n",
    "class DirectionAwareSpatialContextModule(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DirectionAwareSpatialContextModule, self).__init__(**kwargs)\n",
    "        self.attention_network = AttentionEstimatorNetwork()\n",
    "        self.spatial_rnn_cells = {\n",
    "            'right': SpatialRNNCell('right'),\n",
    "            'left': SpatialRNNCell('left'),\n",
    "            'up': SpatialRNNCell('up'),\n",
    "            'down': SpatialRNNCell('down')\n",
    "        }\n",
    "        self.hidden_to_hidden_conv = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention_network.build(input_shape)\n",
    "        for cell in self.spatial_rnn_cells.values():\n",
    "            cell.build(input_shape)\n",
    "        num_channels = self.attention_network.compute_output_shape(input_shape)[-1]\n",
    "        self.hidden_to_hidden_conv = Conv2D(num_channels // 4, (1, 1), activation='relu')\n",
    "        self.hidden_to_hidden_conv.build(input_shape)\n",
    "        super(DirectionAwareSpatialContextModule, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_weights = self.attention_network(inputs)\n",
    "        attention_weights_split = tf.split(attention_weights, num_or_size_splits=4, axis=-1)\n",
    "        \n",
    "        context_features = []\n",
    "        for direction, rnn_cell in self.spatial_rnn_cells.items():\n",
    "            rnn_output = rnn_cell(inputs)\n",
    "            weighted_feature = rnn_output * attention_weights_split.pop(0)\n",
    "            context_features.append(weighted_feature)\n",
    "\n",
    "        concatenated_features = tf.concat(context_features, axis=-1)\n",
    "        output_dsc_features = self.hidden_to_hidden_conv(concatenated_features)\n",
    "        return output_dsc_features\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = (input_shape[0], input_shape[1], input_shape[2], input_shape[3] // 4)\n",
    "        return output_shape\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the VGG16 model\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Initialize the DirectionAwareSpatialContextModule\n",
    "dascm = DirectionAwareSpatialContextModule()\n",
    "\n",
    "# Define the input for the functional model\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Get the feature maps from all layers of VGG16, except the input layer\n",
    "vgg_outputs = [layer.output for layer in vgg.layers[1:]]  # Skip the input layer\n",
    "\n",
    "# Upsample and apply the DirectionAwareSpatialContextModule to each of the feature maps\n",
    "dascm_outputs = []\n",
    "for output in vgg_outputs:\n",
    "    # The output needs to be upsampled to the same size\n",
    "    # Calculate the upsample size for current feature map\n",
    "    upsample_size = (224 // output.shape[1], 224 // output.shape[2])\n",
    "    upsampled_output = UpSampling2D(size=upsample_size)(output)\n",
    "    # Process each upsampled output with the dascm\n",
    "    processed_output = dascm(upsampled_output)\n",
    "    dascm_outputs.append(processed_output)\n",
    "\n",
    "# Concatenate all the DASC module feature maps\n",
    "# We can concatenate along the channel axis as the spatial dimensions are now equal\n",
    "concatenated_outputs = Concatenate(axis=-1)(dascm_outputs)\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=input_tensor, outputs=concatenated_outputs)\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the model summary to check if everything is connected properly\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3268 images belonging to 1 classes.\n",
      "Found 3268 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Define paths to the datasets\n",
    "image_dir = '/Users/hemanthnagulapalli/Desktop/CS512Project/512_course_project/shadow/SBU-shadow/SBUTrain4KRecoveredSmall/ShadowImages'\n",
    "mask_dir = '/Users/hemanthnagulapalli/Desktop/CS512Project/512_course_project/shadow/SBU-shadow/SBUTrain4KRecoveredSmall/ShadowMasks'\n",
    "\n",
    "# Initialize the data generators\n",
    "image_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "mask_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Connect the generators to your dataset\n",
    "# Since we are not classifying images, we set class_mode to None\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    image_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # No class labels\n",
    "    subset='training',\n",
    "    seed=1)\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    mask_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # No class labels\n",
    "    color_mode='grayscale',  # Masks are grayscale\n",
    "    subset='training',\n",
    "    seed=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    # Calculate the weight for each class\n",
    "    Np = tf.reduce_sum(y_true)  # Number of positive samples\n",
    "    Nn = tf.reduce_sum(1 - y_true)  # Number of negative samples\n",
    "    # TN = tf.math.count_nonzero(y_pred * (1 - y_true))\n",
    "    # TP = tf.math.count_nonzero((1 - y_pred) * y_true)\n",
    "    TP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(tf.round(y_pred), 1)), tf.float32))\n",
    "    TN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(tf.round(y_pred), 0)), tf.float32))\n",
    "\n",
    "    \n",
    "    # Weights for balancing the classes\n",
    "    # weight_for_0 = (Nn / (Nn + Np)) * (1 / (1 - TN / Nn))\n",
    "    # weight_for_1 = (Np / (Nn + Np)) * (1 / (1 - TP / Np))\n",
    "    weight_for_0 = (Nn / (Nn + Np)) * (1 / (1 - TN / (Nn + tf.keras.backend.epsilon())))\n",
    "    weight_for_1 = (Np / (Nn + Np)) * (1 / (1 - TP / (Np + tf.keras.backend.epsilon())))\n",
    "\n",
    "    \n",
    "    # Calculate the actual weighted cross-entropy\n",
    "    # The '+ epsilon()' part is to avoid log(0)\n",
    "    loss = -(weight_for_1 * y_true * tf.math.log(y_pred + tf.keras.backend.epsilon()) + \n",
    "             weight_for_0 * (1 - y_true) * tf.math.log(1 - y_pred + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=weighted_cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Output of Layer 1 (block1_conv1): (1, 224, 224, 64)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Output of Layer 2 (block1_conv2): (1, 224, 224, 64)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Output of Layer 3 (block1_pool): (1, 112, 112, 64)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Output of Layer 4 (block2_conv1): (1, 112, 112, 128)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x173824cc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Output of Layer 5 (block2_conv2): (1, 112, 112, 128)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1738c40e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Output of Layer 6 (block2_pool): (1, 56, 56, 128)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Output of Layer 7 (block3_conv1): (1, 56, 56, 256)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Output of Layer 8 (block3_conv2): (1, 56, 56, 256)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Output of Layer 9 (block3_conv3): (1, 56, 56, 256)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Output of Layer 10 (block3_pool): (1, 28, 28, 256)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Output of Layer 11 (block4_conv1): (1, 28, 28, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Output of Layer 12 (block4_conv2): (1, 28, 28, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Output of Layer 13 (block4_conv3): (1, 28, 28, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Output of Layer 14 (block4_pool): (1, 14, 14, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Output of Layer 15 (block5_conv1): (1, 14, 14, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Output of Layer 16 (block5_conv2): (1, 14, 14, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Output of Layer 17 (block5_conv3): (1, 14, 14, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Output of Layer 18 (block5_pool): (1, 7, 7, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "Output of Layer 19 (up_sampling2d): (1, 7, 7, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "Output of Layer 20 (up_sampling2d_1): (1, 7, 7, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
      "Output of Layer 21 (up_sampling2d_2): (1, 14, 14, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Output of Layer 22 (up_sampling2d_3): (1, 28, 28, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "Output of Layer 23 (up_sampling2d_4): (1, 56, 56, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "Output of Layer 24 (up_sampling2d_5): (1, 224, 224, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step\n",
      "Output of Layer 25 (up_sampling2d_6): (1, 896, 896, 512)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Output of Layer 26 (up_sampling2d_7): (1, 3584, 3584, 512)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "import numpy as np\n",
    "\n",
    "# Define the input shape\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Initialize 'x' with the input tensor for applying layers\n",
    "x = input_tensor\n",
    "\n",
    "# Iterate through all layers in the model and build up progressively\n",
    "for i, layer in enumerate(model.layers[1:]):  # Start from the first actual processing layer\n",
    "    try:\n",
    "        # Apply the current layer to the output of the previous layers\n",
    "        x = layer(x)\n",
    "        \n",
    "        # Create a model that includes the input layer up to the current layer\n",
    "        test_model = Model(inputs=input_tensor, outputs=x)\n",
    "        test_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "        # Generate a random test input\n",
    "        test_input = np.random.random((1, 224, 224, 3)).astype('float32')\n",
    "\n",
    "        # Test the model up to the current layer\n",
    "        predictions = test_model.predict(test_input)\n",
    "        print(f\"Output of Layer {i+1} ({layer.name}): {predictions.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at Layer {i+1} ({layer.name}):\", e)\n",
    "        break  # Stop at the first error to fix issues step by step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m10949196624\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\\n  • training=False\\n  • mask=None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m train_generator \u001b[39m=\u001b[39m generate_train_batches(image_generator, mask_generator)\n\u001b[1;32m     12\u001b[0m test_images, test_masks \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(train_generator)\n\u001b[0;32m---> 13\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mpredict(test_images))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/ops/function.py:159\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn)\u001b[0m\n\u001b[1;32m    157\u001b[0m output_tensors \u001b[39m=\u001b[39m []\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs:\n\u001b[0;32m--> 159\u001b[0m     output_tensors\u001b[39m.\u001b[39mappend(tensor_dict[\u001b[39mid\u001b[39m(x)])\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m tree\u001b[39m.\u001b[39mpack_sequence_as(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_struct, output_tensors)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m10949196624\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\\n  • training=False\\n  • mask=None'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def generate_train_batches(image_generator, mask_generator):\n",
    "    while True:\n",
    "        # Get next image and mask batch\n",
    "        image_batch = next(image_generator)\n",
    "        mask_batch = next(mask_generator)\n",
    "        yield (image_batch, mask_batch)\n",
    "\n",
    "# Create a generator\n",
    "train_generator = generate_train_batches(image_generator, mask_generator)\n",
    "\n",
    "test_images, test_masks = next(train_generator)\n",
    "print(model.predict(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m11934120400\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\\n  • training=True\\n  • mask=None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m steps_per_epoch \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mceil(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(image_dir)) \u001b[39m/\u001b[39m \u001b[39m32\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     17\u001b[0m     train_generator,\n\u001b[1;32m     18\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39msteps_per_epoch,\n\u001b[1;32m     19\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m  \u001b[39m# or however many you choose\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/ops/function.py:159\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn)\u001b[0m\n\u001b[1;32m    157\u001b[0m output_tensors \u001b[39m=\u001b[39m []\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs:\n\u001b[0;32m--> 159\u001b[0m     output_tensors\u001b[39m.\u001b[39mappend(tensor_dict[\u001b[39mid\u001b[39m(x)])\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m tree\u001b[39m.\u001b[39mpack_sequence_as(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_struct, output_tensors)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m11934120400\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\\n  • training=True\\n  • mask=None'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def generate_train_batches(image_generator, mask_generator):\n",
    "    while True:\n",
    "        # Get next image and mask batch\n",
    "        image_batch = next(image_generator)\n",
    "        mask_batch = next(mask_generator)\n",
    "        yield (image_batch, mask_batch)\n",
    "\n",
    "# Create a generator\n",
    "train_generator = generate_train_batches(image_generator, mask_generator)\n",
    "\n",
    "# Ensure that image_dir points directly to the folder containing the images\n",
    "steps_per_epoch = math.ceil(len(os.listdir(image_dir)) / 32)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50  # or however many you choose\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (32, 224, 224, 3)\n",
      "Mask batch shape: (32, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the output of the generator\n",
    "image_batch, mask_batch = next(train_generator)\n",
    "print('Image batch shape:', image_batch.shape)\n",
    "print('Mask batch shape:', mask_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
